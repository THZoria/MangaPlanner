#!/usr/bin/env python3
from __future__ import annotations

import asyncio
import csv
import json
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
import argparse
import sys

APP_VERSION = "1.3.0 by Zoria"
BASE_URL = "https://www.nautiljon.com"
PLANNING_URL = f"{BASE_URL}/planning/manga/"

# ---------------------------  Models ---------------------------

@dataclass
class MangaItem:
    nom_manga: str
    date_sortie: str
    prix: str
    editeur: Optional[str]
    lien_acheter: Optional[str]
    image: Optional[str]

    def to_dict(self) -> dict:
        return asdict(self)


# --------------------------- Export helpers ---------------------------

def export_json(items: List[MangaItem], out_path: Path) -> None:
    with out_path.open("w", encoding="utf-8") as f:
        json.dump([i.to_dict() for i in items], f, ensure_ascii=False, indent=2)


def export_csv(items: List[MangaItem], out_path: Path) -> None:
    fieldnames = ["nom_manga", "date_sortie", "prix", "editeur", "lien_acheter", "image"]
    with out_path.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for it in items:
            writer.writerow(it.to_dict())


# --------------------------- Scraper core ---------------------------

async def dismiss_gdpr(page) -> None:
    """
    Essaie plusieurs variantes de boutons/roles pour fermer une √©ventuelle popup RGPD.
    Silencieux en cas d'√©chec : on continue sans bloquer.
    """
    candidates = [
        # Boutons fr√©quents FR
        {"role": "button", "name": r"Continuer sans accepter"},
        {"role": "button", "name": r"Tout refuser|Refuser tout"},
        {"role": "button", "name": r"Continuer|Fermer|Accepter et fermer"},
        # Lien possible
        {"role": "link", "name": r"Continuer sans accepter|Tout refuser|Fermer"},
    ]
    for c in candidates:
        try:
            await page.get_by_role(c["role"], name=c["name"], exact=False).click(timeout=1500)
            logging.info("‚úÖ Pop-up RGPD ignor√©e (%s / %s).", c["role"], c["name"])
            return
        except Exception:
            continue
    logging.info("‚ÑπÔ∏è Aucune pop-up RGPD √† fermer (ou non d√©tect√©e).")


async def extract_planning(page) -> List[MangaItem]:
    """
    Extrait toutes les lignes du tableau via un evaluate JS c√¥t√© page
    pour minimiser les allers-retours Python <-> navigateur.
    """
    try:
        await page.wait_for_selector("#planning tbody", timeout=15_000)
        logging.info("‚úÖ Tableau de planning d√©tect√©.")
    except PlaywrightTimeout:
        raise RuntimeError("Le tableau #planning n'a pas √©t√© trouv√© (timeout).")

    rows = await page.eval_on_selector_all(
        "#planning tbody tr",
        """
        (trs) => trs.map(tr => {
            const tds = Array.from(tr.querySelectorAll('td'));
            if (tds.length < 6) return null;

            const text = el => (el?.textContent || '').trim();

            const date_sortie = text(tds[0]);

            const imgEl = tds[1].querySelector('a img');
            let image = imgEl ? (imgEl.getAttribute('src') || '').trim() : null;
            if (image && !/^https?:\\/\\//.test(image)) image = 'https://www.nautiljon.com' + image;

            const linksTitle = tds[2].querySelectorAll('a');
            const nom_manga = linksTitle.length ? text(linksTitle[linksTitle.length - 1]) : text(tds[2]);

            const prix = text(tds[3]);

            const edLink = tds[4].querySelector('a');
            const editeur = edLink ? text(edLink) : (text(tds[4]) || null);

            const buyLink = tds[5].querySelector('a');
            let lien_acheter = buyLink ? (buyLink.getAttribute('href') || '').trim() : null;
            if (lien_acheter && !/^https?:\\/\\//.test(lien_acheter)) lien_acheter = 'https://www.nautiljon.com' + lien_acheter;

            return { nom_manga, date_sortie, prix, editeur, lien_acheter, image };
        }).filter(Boolean)
        """,
    )

    items: List[MangaItem] = []
    for r in rows:
        
        nom = (r.get("nom_manga") or "").strip()
        if not nom:
            continue
        items.append(
            MangaItem(
                nom_manga=nom,
                date_sortie=(r.get("date_sortie") or "").strip(),
                prix=(r.get("prix") or "").strip(),
                editeur=(r.get("editeur") or None) or None,
                lien_acheter=(r.get("lien_acheter") or None) or None,
                image=(r.get("image") or None) or None,
            )
        )
    return items


async def scrape(headless: bool, timeout: int, debug: bool) -> List[MangaItem]:
    launch_kwargs = dict(headless=headless, args=["--no-sandbox"])
    if debug:
        
        launch_kwargs["slow_mo"] = 50

    async with async_playwright() as p:
        browser = await p.chromium.launch(**launch_kwargs)
        try:
            context = await browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/122.0.0.0 Safari/537.36"
                ),
                java_script_enabled=True,
                viewport={"width": 1400, "height": 900},
            )
            page = await context.new_page()
            logging.info("üåê Acc√®s √† la page‚Ä¶ %s", PLANNING_URL)
            await page.goto(PLANNING_URL, wait_until="domcontentloaded", timeout=timeout * 1000)

            
            await page.wait_for_timeout(800)
            await dismiss_gdpr(page)

            items = await extract_planning(page)
            logging.info("‚úÖ %d mangas r√©cup√©r√©s.", len(items))
            return items
        finally:
            await browser.close()


# --------------------------- CLI ---------------------------

def parse_args(argv: List[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Scraper Nautiljon planning manga (modernis√©)."
    )
    parser.add_argument(
        "--out",
        type=Path,
        default=Path(f"planning_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"),
        help="Chemin du fichier de sortie (par d√©faut: planning_YYYYmmdd_HHMMSS.json)",
    )
    parser.add_argument(
        "--format",
        choices=["json", "csv"],
        default="json",
        help="Format de sortie (json ou csv).",
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=30,
        help="Timeout de chargement initial de la page (secondes).",
    )
    
    headless_group = parser.add_mutually_exclusive_group()
    headless_group.add_argument("--headless", dest="headless", action="store_true", help="Mode headless (par d√©faut).")
    headless_group.add_argument("--no-headless", dest="headless", action="store_false", help="Affiche le navigateur.")
    parser.set_defaults(headless=True)

    parser.add_argument(
        "--debug",
        action="store_true",
        help="Active des logs plus verbeux et slow-mo navigateur.",
    )

    return parser.parse_args(argv)


def setup_logging(debug: bool) -> None:
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )


# --------------------------- Entry point ---------------------------

async def amain(argv: List[str]) -> int:
    args = parse_args(argv)
    setup_logging(args.debug)

    logging.info("Version : %s", APP_VERSION)
    try:
        items = await scrape(headless=args.headless, timeout=args.timeout, debug=args.debug)
    except Exception as e:
        logging.error("‚ùå √âchec du scraping : %s", e, exc_info=args.debug)
        return 1

    if not items:
        logging.warning("‚ö†Ô∏è Aucun manga trouv√©. Le site a peut-√™tre chang√©.")
    else:
        out: Path = args.out
        out.parent.mkdir(parents=True, exist_ok=True)
        if args.format == "json":
            export_json(items, out)
        else:
            export_csv(items, out)
        logging.info("üíæ Export %s -> %s", args.format.upper(), out.resolve())

    return 0


def main() -> None:
    try:
        exit_code = asyncio.run(amain(sys.argv[1:]))
    except KeyboardInterrupt:
        logging.warning("Interrompu par l'utilisateur.")
        exit_code = 130
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
